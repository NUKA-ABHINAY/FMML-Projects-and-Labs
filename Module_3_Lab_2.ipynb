{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv9ZVLt1EWiFzgXAAo4ZAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NUKA-ABHINAY/FMML-Projects-and-Labs/blob/main/Module_3_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTIONS AND ANSWERS :\n",
        "-----------------------\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IT7f9SQebPAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)  In the section on Decision boundaries, you must have seen that we ran the KNN algorithm twice: first with the weights set to 'uniform' and then set to 'distance'. Find out the difference between these two.\n",
        "\n",
        "Answer:\n",
        "-------\n",
        "The parameter you refer to ('uniform' versus 'distance') in the context of the k-nearest neighbors (KNN) algorithm has to do with how the algorithm weights the neighboring points when making predictions.\n",
        "\n",
        "Uniform Weighing :\n",
        "-----------------\n",
        "\n",
        "When 'uniform' is selected as the parameter, every neighboring point receives the same weight when making decisions. Put differently, irrespective of their distance from the point under prediction, every k-nearest neighbor makes an equal contribution to the prediction.\n",
        "\n",
        "Weighting by Distance :\n",
        "---------------------\n",
        "When the 'distance' parameter is set, neighbors that are closer to you affect the prediction more than neighbors that are farther away. The premise is that points nearer the query point are probably more relevant, and as a result, their contribution to the prediction is given a higher weight. In conclusion, the distinction is in the way the algorithm weighs the significance of nearby points. While 'distance' assigns greater weight to neighbors that are closer together, 'uniform' assigns equal weight to all neighbors. The model's behavior and performance can be affected by the selection of one of these options, and the best option frequently depends on the particulars of the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nfhN6JlIbX7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)What do you think could be the drawbacks of using KNN ?\n",
        "\n",
        "Answer:\n",
        "\n",
        "While the k-nearest neighbors (KNN) algorithm is simple and intuitive, it also has several drawbacks and limitations that may make it less suitable for certain scenarios. Here are some drawbacks of using KNN:\n",
        "\n",
        "1)  Computational Cost:\n",
        " As the size of the dataset increases, the computational cost of KNN also increases. This is because, during prediction, the algorithm needs to calculate the distances between the query point and all data points in the dataset.\n",
        "\n",
        "2)  Memory Usage:\n",
        " KNN requires storing the entire dataset in memory, as it needs to access and compare each data point during prediction. For large datasets, this can be memory-intensive.\n",
        "\n",
        "3)  Sensitivity to Outliers:\n",
        "KNN is sensitive to outliers and noise in the data. Outliers can significantly impact the decision boundaries and lead to incorrect predictions.\n",
        "\n",
        "4)  Curse of Dimensionality:\n",
        "In high-dimensional spaces, the concept of proximity becomes less meaningful. This is known as the curse of dimensionality, and it can lead to degraded performance of KNN as the number of features increases.\n",
        "\n",
        "5)  Need for Feature Scaling:\n",
        " KNN is sensitive to the scale of features. Features with larger scales can dominate the distance calculation, leading to biased results. Therefore, it's often necessary to normalize or standardize the features.\n",
        "\n",
        "6)  Choosing an Optimal K:\n",
        "The performance of KNN is highly dependent on the choice of the parameterk (the number of neighbors). Selecting an appropriate k value can be challenging and may require experimentation.\n",
        "\n",
        "7)  Imbalanced Datasets:\n",
        "KNN may struggle with imbalanced datasets where one class significantly outnumbers the others. The majority class can dominate the predictions, leading to poor performance on minority classes.\n",
        "\n",
        "8)  No Model Representation: KNN does not learn an explicit model during training, making it less interpretable. It doesn't provide insights into which features are most important for predictions.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S0_Heq7TcD9S"
      }
    }
  ]
}